{
  "name": "Practial machine learning poject assignment",
  "tagline": "Repo for project assignment for Coursera course Practical Machine Learning",
  "body": "# Practical Machine Learning by Coursera - Project Assignment\r\nIgor Hut  \r\n01 jun 2016   \r\n\r\n## Introduction  \r\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.  \r\n\r\nIn this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information regarding the whole experiment is available from the website here: <http://groupware.les.inf.puc-rio.br/har>. \r\n\r\n## Initial preparation - Loading necessary packages and basic data preparation \r\n\r\n```r\r\nlibrary(caret)\r\nlibrary(randomForest)\r\nlibrary(rpart)\r\nlibrary(rpart.plot)\r\n```\r\n### Downloading the data\r\n\r\n```r\r\ntrainDataUrl <-\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\ntestDataUrl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\ntrainData <- \"./data/pml-training.csv\"\r\ntestData  <- \"./data/pml-testing.csv\"\r\nif (!file.exists(\"./data\")) {\r\n  dir.create(\"./data\")\r\n}\r\nif (!file.exists(trainData)) {\r\n  download.file(trainDatUrl, destfile=trainData)\r\n}\r\nif (!file.exists(testData)) {\r\n  download.file(testDataUrl, destfile=testData)\r\n}\r\n```\r\n### Reading in and checking the data\r\nAfter downloading the data from the data source, we can read the two csv files into two data frames.  \r\n\r\n```r\r\ntrainDataRaw <- read.csv(\"./data/pml-training.csv\")\r\ntestDataRaw <- read.csv(\"./data/pml-testing.csv\")\r\ndim(trainDataRaw)\r\n```\r\n\r\n```\r\n## [1] 19622   160\r\n```\r\n\r\n```r\r\ndim(testDataRaw)\r\n```\r\n\r\n```\r\n## [1]  20 160\r\n```\r\nAs can be observed the training data set contains 19622 observations and 160 variables, while the testing data set contains 20 observations and 160 variables. The \"classe\" variable in the training set is the outcome to predict. \r\n\r\n### Spliting the data set\r\n\r\nWe are going to split the training data set into a pure training data set (70%) and a validation data set (30%).The former will be used to perform cross validation in forthcoming steps.  \r\n\r\n\r\n```r\r\nset.seed(33)\r\ninTrain <- createDataPartition(y=trainDataRaw$classe, p=0.7, list=F)\r\ntrainSet <-trainDataRaw[inTrain, ]\r\ntestSet<- trainDataRaw[-inTrain, ]\r\n```\r\n\r\n### Cleaning the data\r\n\r\nIn this section we are going to reduce the number of features by removing variables with nearly zero variance, variables that are almost always NA, as well as variables that wouldn't play any meaningful role in the prediction task. \r\n\r\n\r\n```r\r\n# Removing vars with nearly zero variance\r\nnzv <- nearZeroVar(trainSet)\r\ntrainSet <- trainSet[, -nzv]\r\ntestSet <- testSet[, -nzv]\r\n\r\n# Removing vars that are almost always NA\r\nmostlyNA <- sapply(trainSet, function(x) mean(is.na(x))) > 0.95\r\ntrainSet <- trainSet[, mostlyNA==F]\r\ntestSet <- testSet[, mostlyNA==F]\r\n\r\n# Removing variables that don't play any meaningful role in the prediction task (X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp). These are the first five variables in the given data set.\r\n\r\ntrainSet <- trainSet[, -(1:5)]\r\ntestSet <- testSet[, -(1:5)]\r\n\r\ndim(trainSet)\r\n```\r\n\r\n```\r\n## [1] 13737    54\r\n```\r\n\r\n```r\r\ndim(testSet)\r\n```\r\n\r\n```\r\n## [1] 5885   54\r\n```\r\nNow, the cleaned training data set contains 13737 observations and 54 variables, while the cross validation testing data set contains 5885 observations and 54 variables. The \"classe\" variable is still in the cleaned training set.\r\n\r\n\r\n## Data modeling\r\n\r\nFor the begining we will choose the **Random Forest** ML algorithm to fit a predictive model for activity recognition, and check how it behaves. This is our first choice since **RF** automatically selects important variables and is quite robust to correlated covariates & outliers in general. We will use **5-fold cross validation** when applying the algorithm.  \r\n\r\n\r\n```r\r\n# Train function will be using 5-fold CV to select optimal tuning parameters\r\ntrainFit <- trainControl(method=\"cv\", number=5, verboseIter=F)\r\n\r\n# Fitting the model on trainSet\r\nmodelFit <- train(classe ~ ., data=trainSet, method=\"rf\", trControl=trainFit)\r\n\r\n# Print final model to check which tuning parameters it chose\r\nmodelFit$finalModel\r\n```\r\n\r\n```\r\n## \r\n## Call:\r\n##  randomForest(x = x, y = y, mtry = param$mtry) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 500\r\n## No. of variables tried at each split: 27\r\n## \r\n##         OOB estimate of  error rate: 0.28%\r\n## Confusion matrix:\r\n##      A    B    C    D    E  class.error\r\n## A 3905    1    0    0    0 0.0002560164\r\n## B    3 2650    4    1    0 0.0030097818\r\n## C    0    9 2386    1    0 0.0041736227\r\n## D    0    0   11 2240    1 0.0053285968\r\n## E    0    2    0    6 2517 0.0031683168\r\n```\r\n\r\nIt can be seen that it decided to use 500 trees and try 27 variables at each split.\r\n\r\nFurther, we will estimate the performance of the model on the validation data set. \r\n\r\n\r\n\r\n### Model evaluation \r\n\r\nWe are using the fitted model to predict the label (“classe”) in `testSet`. Confusion matrix enables us to compare the predicted versus the actual labels.\r\n\r\n\r\n```r\r\npreds <- predict(modelFit, newdata=testSet)\r\n\r\n# Show the confusion matrix to get estimates of the accuracy and out-of-sample error\r\nconfusionMatrix(testSet$classe, preds)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1673    0    0    0    1\r\n##          B    2 1137    0    0    0\r\n##          C    0    1 1025    0    0\r\n##          D    0    0    9  955    0\r\n##          E    0    0    0    0 1082\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9978          \r\n##                  95% CI : (0.9962, 0.9988)\r\n##     No Information Rate : 0.2846          \r\n##     P-Value [Acc > NIR] : < 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9972          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9988   0.9991   0.9913   1.0000   0.9991\r\n## Specificity            0.9998   0.9996   0.9998   0.9982   1.0000\r\n## Pos Pred Value         0.9994   0.9982   0.9990   0.9907   1.0000\r\n## Neg Pred Value         0.9995   0.9998   0.9981   1.0000   0.9998\r\n## Prevalence             0.2846   0.1934   0.1757   0.1623   0.1840\r\n## Detection Rate         0.2843   0.1932   0.1742   0.1623   0.1839\r\n## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839\r\n## Balanced Accuracy      0.9993   0.9993   0.9955   0.9991   0.9995\r\n```\r\n\r\nSo, the estimated accuracy of the model is 99.72% and the estimated out-of-sample error is 0.28%.\r\nThis is quite a promising result, thus we will use **RF** algorithm to perform prediction on the given test set of 20 cases.\r\n\r\n### Decision tree visualization\r\n\r\n\r\n```r\r\ntree <- rpart(classe ~ ., data=trainSet, method=\"class\")\r\nprp(tree) # this \"fast\" plot is better for visualisation of a complex tree than i.e.               fancyRpartPlot which would be crammed\r\n```\r\n\r\n![](Practical_Mach_Learing_Proj_Assign_files/figure-html/unnamed-chunk-8-1.png)\r\n\r\n## Predicting for test data set\r\n\r\n### Retraining the model\r\n\r\nBefore predicting on the test set, we will train the model, again, but this time on the full training set (`trainDataRaw`), rather than use a model obtained by fitting the reduced training set (`trainSet`). This should lead to more accurate predictions. Therefore we will repeat the whole data preprocessing procedure with `trainDataRaw` and `testDataRaw`.\r\n\r\n\r\n\r\n```r\r\n# Removing vars with nearly zero variance\r\nnzv <- nearZeroVar(trainDataRaw)\r\ntrainFinal <- trainDataRaw[, -nzv]\r\ntestFinal <- testDataRaw[, -nzv]\r\n\r\n# Removing vars that are almost always NA\r\nmostlyNA <- sapply(trainFinal, function(x) mean(is.na(x))) > 0.95\r\ntrainFinal <- trainFinal[, mostlyNA==F]\r\ntestFinal <- testFinal[, mostlyNA==F]\r\n\r\n# Removing variables that don't play any meaningful role in the prediction task (X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp). These are the first five variables in the given data set.\r\n\r\ntrainFinal <- trainFinal[, -(1:5)]\r\ntestFinal <- testFinal[, -(1:5)]\r\n\r\n# Re-train the model with the full training set\r\n\r\ntrainFit <- trainControl(method=\"cv\", number=5, verboseIter=F)\r\nmodelFinal <- train(classe ~ ., data=trainFinal, method=\"rf\", trControl=trainFit)\r\n```\r\n\r\n### Aplying the final model for prediction on the test data set\r\n\r\n\r\n```r\r\npreds <- predict(modelFinal, newdata=testFinal)\r\npreds\r\n```\r\n\r\n```\r\n##  [1] B A B A A E D B A A B C B A E E A B B B\r\n## Levels: A B C D E\r\n```\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}